{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVaF-vjM1xe-"
   },
   "source": [
    "# **Black Friday Sales Prediction**\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "* With increasing sales of electronic goods and other provisional items during Black friday, it is important for every product chains like Walmart, BestBuy, Target etc. to know about the sale of every products based on their purchase history. \n",
    "\n",
    "* It will greatly help the organization chains to equip the products more efficiently based on quantity of products that had been purchased in past years. It not only helps in upkeeping the stocks of products which has been selling more, but also it will help promote the items which are not sold up to target. Analysing the data history about the product sales will greatly help in shrinking the bridge between costumer needs and product chains.\n",
    "\n",
    "* It will also help the supply chains to improve the efficiency of target marketing based on different features of the dataset.\n",
    "\n",
    "* With this dataset, we will be able to provide answers to following questions about the Black Friday Sales.\n",
    "        1. How do people with different age genders and marital status purchase products on black friday?\n",
    "        2. What is the influence of people's occupation in Black Friday sales?\n",
    "        3. Do young people purchase more than older people in Black Friday?\n",
    "        4. Which product categories are sold predominantly than others?\n",
    "        5. Are the customers who are buying more in Black Friday, new to the city or are they staying in the same city for more years?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:37:40.945956Z",
     "iopub.status.busy": "2022-10-18T01:37:40.945555Z",
     "iopub.status.idle": "2022-10-18T01:37:40.957681Z",
     "shell.execute_reply": "2022-10-18T01:37:40.956567Z",
     "shell.execute_reply.started": "2022-10-18T01:37:40.945923Z"
    },
    "id": "jV81GAXf1xfB"
   },
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import gc\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9KNkYkF1xfB"
   },
   "source": [
    "## About Dataset:\n",
    "\n",
    "The source of dataset is from Kaggle (https://www.kaggle.com/datasets/pranavuikey/black-friday-sales-eda). Dataset includes 550068 data records with 12 columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:05.246075Z",
     "iopub.status.busy": "2022-10-18T01:36:05.245661Z",
     "iopub.status.idle": "2022-10-18T01:36:05.700352Z",
     "shell.execute_reply": "2022-10-18T01:36:05.698405Z",
     "shell.execute_reply.started": "2022-10-18T01:36:05.246041Z"
    },
    "id": "a2BZH0tW1xfC",
    "outputId": "64fa254e-61b1-41fc-f584-db50fd74eebf"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\src\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8116bd157a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading the dataset using pandas and displaying top 5 records of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\src\\train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape: {df.shape}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\src\\train.csv'"
     ]
    }
   ],
   "source": [
    "# reading the dataset using pandas and displaying top 5 records of dataset\n",
    "df = pd.read_csv('\\src\\train.csv')\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:05.704728Z",
     "iopub.status.busy": "2022-10-18T01:36:05.704330Z",
     "iopub.status.idle": "2022-10-18T01:36:05.827248Z",
     "shell.execute_reply": "2022-10-18T01:36:05.825634Z",
     "shell.execute_reply.started": "2022-10-18T01:36:05.704696Z"
    },
    "id": "XgOzjKy21xfC",
    "outputId": "670350b8-7ebd-41b1-ca98-550a7f248af1"
   },
   "outputs": [],
   "source": [
    "# Mapping the age feature in the readable way\n",
    "print(f\"Mapping Step.......\\n\")\n",
    "print(f\"\\nValues in Ages before mapping: {df['Age'].unique()}\\n\")\n",
    "age_dict = {'0-17': 'Child(Under 17)', '18-25': 'Young Adult (18-25)', '26-35': 'Mid Adult (26-35)', \\\n",
    "           '36-45': 'Adult Old (36-45)', '46-50': 'Old (46-50)', '51-55': 'Old Above 50', \\\n",
    "           '55+': 'Older above 55'}\n",
    "df['Age'] = df['Age'].map(age_dict)\n",
    "print(f\"\\nValues in Ages after mapping: {df['Age'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:05.830767Z",
     "iopub.status.busy": "2022-10-18T01:36:05.830370Z",
     "iopub.status.idle": "2022-10-18T01:36:06.034184Z",
     "shell.execute_reply": "2022-10-18T01:36:06.032672Z",
     "shell.execute_reply.started": "2022-10-18T01:36:05.830733Z"
    },
    "id": "EPFmlERe1xfC",
    "outputId": "5022375d-351b-4275-e99a-6c89a94e77d8"
   },
   "outputs": [],
   "source": [
    "# Removing '+' at the end of '4+' in Stay_In_Current_City_Years feature\n",
    "print(f\"\\nRemoving '+' at the end of 4+ character in Stay_In_Current_City_Years.....\\n\")\n",
    "print(f\"\\nUnique values in Stay_In_Current_City_Years before removing +.....{df['Stay_In_Current_City_Years'].unique()}\\n\")\n",
    "df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].apply(lambda x: x.replace('+',''))\n",
    "print(f\"\\nUnique values in Stay_In_Current_City_Years after removing +.....{df['Stay_In_Current_City_Years'].unique()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:06.035673Z",
     "iopub.status.busy": "2022-10-18T01:36:06.035308Z",
     "iopub.status.idle": "2022-10-18T01:36:06.175685Z",
     "shell.execute_reply": "2022-10-18T01:36:06.174774Z",
     "shell.execute_reply.started": "2022-10-18T01:36:06.035637Z"
    },
    "id": "aTOqt0n41xfC",
    "outputId": "8080a6cb-a395-4ff5-ebb8-4f36edccdbc8"
   },
   "outputs": [],
   "source": [
    "# Updating Marital Status as Single and Married\n",
    "print(f\"\\nUpdating Marital Status as Single and Married...\\n\")\n",
    "print(f\"\\nUnique values in Marital_Status before updating +.....{df['Marital_Status'].unique()}\\n\")\n",
    "df['Marital_Status']=df['Marital_Status'].apply(lambda x: 'Married' if x==1  else 'Single')\n",
    "print(f\"\\nUnique values in Marital_Status after updating +.....{df['Marital_Status'].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:06.178275Z",
     "iopub.status.busy": "2022-10-18T01:36:06.176979Z",
     "iopub.status.idle": "2022-10-18T01:36:06.198731Z",
     "shell.execute_reply": "2022-10-18T01:36:06.197829Z",
     "shell.execute_reply.started": "2022-10-18T01:36:06.178202Z"
    },
    "id": "4-GT28ar1xfD"
   },
   "outputs": [],
   "source": [
    "# This class is to understand the dataset more with separate dataframe and plots\n",
    "class understanding_data:\n",
    "    @classmethod\n",
    "    def understand_data(self, df, perc_limit, n=10, col=None):\n",
    "        '''\n",
    "        This function is to primary function of the class which call all other sub functions in it to \n",
    "        visualize and print the summary data frame.\n",
    "        :param df: DataFrame to which we need summary of to get the understanding.\n",
    "        :param perc_limit: Percentage of null values in any features that we need to filter. Eg.: 0.01 for 1%.\n",
    "        :param n: Default to 10. Denotes number of columns in output dataframe result.\n",
    "        :param col: Default to None. If any particular column result is needed.\n",
    "        :return summaryDF: DataFrame which includes 'column_name', 'column_dtype', 'unique_value_count', 'na_count'and 'na_percentage'.\n",
    "        :return summaryList: List which includes all the fields of SummaryDF.\n",
    "        :return df_miss: DataFrame which only includes features with missing values in it.\n",
    "        '''\n",
    "        summaryDF, summarList = self.summarise_col(df, col, n)\n",
    "        df_miss = self.na_count(df, perc_limit)\n",
    "        self.visualize_understanding(summaryDF)\n",
    "        self.visualization_na(df_miss)\n",
    "        return summaryDF, summarList, df_miss\n",
    "    \n",
    "    @classmethod\n",
    "    def visualize_understanding(self, summaryDF):\n",
    "        '''\n",
    "        This function is to visualize the different data types of features in data frame\n",
    "        :param summaryDF: DataFrame which includes 'column_name', 'column_dtype', 'unique_value_count', 'na_count'and 'na_percentage'. Output of function summarise_col.\n",
    "        :returns Pie chart for distribution of data types in dataset.\n",
    "        '''\n",
    "        fig, ax = plt.subplots(figsize=(7,7))\n",
    "        rects = summaryDF.column_dtype.value_counts().plot.pie(explode=[0.01,0.01,0.01],autopct='%1.2f%%',shadow=True)\n",
    "        plt.title('Data types in Dataset')\n",
    "        plt.show()\n",
    "    \n",
    "    @classmethod\n",
    "    def summarise_col(self, df, column=None, n=10):\n",
    "        '''\n",
    "        This function is to create the summary data frame.\n",
    "        :param df: DataFrame to which we need summary of to get the understanding.\n",
    "        :param n: Default to 10. Denotes number of columns in output dataframe result.\n",
    "        :param col: Default to None. If any particular column result is needed.\n",
    "        :return summaryDF: DataFrame which includes 'column_name', 'column_dtype', 'unique_value_count', 'na_count'and 'na_percentage'.\n",
    "        :return summaryList: List which includes all the fields of SummaryDF.\n",
    "        '''\n",
    "        summaryList = []\n",
    "        if column is None:\n",
    "            for col in df.columns:\n",
    "                summaryList.append([col, df[col].dtype , df[col].nunique(), df[col].isna().sum(), df[col].isna().sum()/df.shape[0]])\n",
    "        else:\n",
    "            for col in column:\n",
    "                summaryList.append([col, df[col].dtype(), df[col].nunique(), df[col].value_counts(), df[col].isna().sum(), df[col].isna().sum()/df.shape[0]])\n",
    "        summaryDF = pd.DataFrame(summaryList[0: n+1])\n",
    "        summaryDF.columns = ['column_name', 'column_dtype', 'unique_value_count', 'na_count', 'na_percentage']\n",
    "        summaryDF.sort_values(by = 'na_count', ascending=False, inplace=True)\n",
    "        return summaryDF, summaryList\n",
    "\n",
    "    @classmethod\n",
    "    def visualization_na(self, df_miss):\n",
    "        '''\n",
    "        This function is to visualize the different missing values of different features in dataframe.\n",
    "        :param df_miss: DataFrame which includes 'feature_name', 'miss_value_count' and 'miss_value_percent'. Output of function na_count.\n",
    "        :returns Pie chart for distribution of missing values in dataset.\n",
    "        '''\n",
    "        df_miss = df_miss[['miss_value_count']].drop_duplicates()\n",
    "        ind = np.arange(df_miss.shape[0])\n",
    "        width = 0.2\n",
    "        fig, ax = plt.subplots(figsize=(7,7))\n",
    "        # rects = ax.pie(ind, df_miss.miss_value_count.values)\n",
    "        rects = df_miss.miss_value_count.plot.pie(autopct='%1.2f%%',shadow=True)\n",
    "        ax.set_title(\"Missing Value Summary of various features\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def na_count(self, df, perc_limit):\n",
    "        '''\n",
    "        This function is to obtain the data frame for features with missing values in it.\n",
    "        :param df: DataFrame to which we need missing value distribution.\n",
    "        :param perc_limit: Percentage of null values in any features that we need to filter. Eg.: 0.01 for 1%.\n",
    "        :return df_miss: DataFrame which only includes features with missing values in it.\n",
    "        '''\n",
    "        df = df.copy()\n",
    "        miss_val_cnt = df.isnull().sum().sort_values(ascending = False)\n",
    "        miss_val_cnt_pcnt = (df.isnull().sum()/df.shape[0]).sort_values(ascending = False)\n",
    "        df_na = pd.concat([miss_val_cnt, miss_val_cnt_pcnt], axis=1, join='outer', keys = ['miss_value_count', 'miss_value_percent'])\n",
    "        df_na.index.name = 'feature_name'\n",
    "        df_na.reset_index()\n",
    "        df_na = df_na.sort_values(by = 'miss_value_count')\n",
    "        df_na = df_na[df_na['miss_value_percent'] > perc_limit]\n",
    "        return df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:06.200082Z",
     "iopub.status.busy": "2022-10-18T01:36:06.199736Z",
     "iopub.status.idle": "2022-10-18T01:36:07.256741Z",
     "shell.execute_reply": "2022-10-18T01:36:07.256004Z",
     "shell.execute_reply.started": "2022-10-18T01:36:06.200053Z"
    },
    "id": "iJjUczAU1xfD",
    "outputId": "f2a54eae-3c73-4068-b92b-751b59519d06"
   },
   "outputs": [],
   "source": [
    "summaryDf,_,df_miss = understanding_data.understand_data(df, 0.001, n=len(df.columns))\n",
    "summaryDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpC9BTIq1xfD"
   },
   "source": [
    "**Missing Value**\n",
    "Columns *Product_Category_3* and *Product_Category_2* are the only features with missing values \n",
    "in it. It consitutes around 70% and 31% missing values respectively. \n",
    "\n",
    "**Unique Value Counts**\n",
    "Almost most of the features available in the dataset are categorical variables. We have to apply either one-hot encoding or label encoding in order to make it as interger or float to make it easier for modelling ml algorithms.\n",
    "* **One Hot Encoding:** \n",
    "* **Label Encoding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:07.258852Z",
     "iopub.status.busy": "2022-10-18T01:36:07.257841Z",
     "iopub.status.idle": "2022-10-18T01:36:07.273675Z",
     "shell.execute_reply": "2022-10-18T01:36:07.272892Z",
     "shell.execute_reply.started": "2022-10-18T01:36:07.258822Z"
    },
    "id": "8iI0rcWU1xfD"
   },
   "outputs": [],
   "source": [
    "# This class is to get the unique calues in different categorical features of dataset\n",
    "class categorical_nuniq_identification:\n",
    "    \n",
    "    @classmethod\n",
    "    def cat_nuniq_less_than_10(self, df, n=10):\n",
    "        '''\n",
    "        This function is to primary function of the class which call all other sub functions in it to \n",
    "        visualize and unique values of categorical features in dataset.\n",
    "        :param df: DataFrame to which we need summary of unique values in different features of dataset.\n",
    "        :param n: Default to 10. Denotes number of columns in output dataframe result.\n",
    "        :return df_nuniq: DataFrame which only includes unique values of different features in dataset.\n",
    "        '''\n",
    "        df = df.copy()\n",
    "        cat_col = self.identifying_cat_columns(df)\n",
    "        df_nuniq = self.get_cat_nuniq_less_than_10(df, cat_col, n)\n",
    "        self.visualizing_cat_nuniq(df_nuniq)\n",
    "        return df_nuniq\n",
    "    \n",
    "    @classmethod\n",
    "    def identifying_cat_columns(self, df):\n",
    "        '''\n",
    "        This function is to identify categorical columns in dataset.\n",
    "        :param df: DataFrame to which we need summary of unique values in different features of dataset.\n",
    "        :return cat_columns: List of categorical features in dataset.\n",
    "        '''\n",
    "        num_columns = [col for col in df.columns if df[col].dtype in ('int', 'float')]\n",
    "        cat_columns = list(set(df.columns) - set(num_columns))\n",
    "        return cat_columns\n",
    "    \n",
    "    @classmethod\n",
    "    def visualizing_cat_nuniq(self, df_nuniq, n=10):\n",
    "        '''\n",
    "        This function is to visualize and unique values of categorical features in dataset.\n",
    "        :param df_nuniq: DataFrame which only includes unique values of different features in dataset. Output of function get_cat_nuniq_less_than_10\n",
    "        :param n: Default to 10. Denotes number of columns in output dataframe result.\n",
    "        :returns visualization plot\n",
    "        '''\n",
    "        df_nuniq = df_nuniq[df_nuniq['nunique_count'] <= n ]\n",
    "        ind = np.arange(df_nuniq.shape[0])\n",
    "        width = 0.2\n",
    "        fig, ax = plt.subplots(figsize=(15,4))\n",
    "        rects = ax.barh(ind, df_nuniq.nunique_count.values)\n",
    "        ax.set_yticks(ind)\n",
    "        ax.set_yticklabels(df_nuniq.index.values, rotation='horizontal')\n",
    "        ax.set_xlabel(\"Unique value Count\")\n",
    "        ax.set_title(\"Features\")\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def get_cat_nuniq_less_than_10(self, df, cat_col, n=10):\n",
    "        '''\n",
    "        This function is to get unique values of categorical features in dataset.\n",
    "        :param df: DataFrame to which we need summary of unique values in different features of dataset.\n",
    "        :param cat_col: List of categorical columns in dataset.\n",
    "        :param n: Default to 10. Denotes number of columns in output dataframe result.\n",
    "        :return df_nuniq: DataFrame which only includes unique values of different features in dataset.\n",
    "        '''\n",
    "        df = df.copy()\n",
    "        total = df.shape[0]\n",
    "        nuniq_list = []\n",
    "        for col in df.columns[0:len(df.columns)-1]:\n",
    "            nuniq = df[col].nunique()\n",
    "            if nuniq <= n:\n",
    "                if col in cat_col:\n",
    "                    nuniq_list.append([col, 'categorical_column', nuniq, df[col].unique()])\n",
    "            else:\n",
    "                nuniq_list.append([col, 'numerical_column', nuniq, df[col].unique()])\n",
    "        df_nuniq = pd.DataFrame(nuniq_list, columns = ['feature', 'column_type', 'nunique_count', 'unique_values_in_feature'])\n",
    "        df_nuniq = df_nuniq.set_index('feature')\n",
    "        df_nuniq.reset_index()\n",
    "        df_nuniq = df_nuniq.sort_values(by = ['nunique_count', 'column_type'] , ascending = [False, True])\n",
    "        return df_nuniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:07.275226Z",
     "iopub.status.busy": "2022-10-18T01:36:07.274692Z",
     "iopub.status.idle": "2022-10-18T01:36:08.025419Z",
     "shell.execute_reply": "2022-10-18T01:36:08.023937Z",
     "shell.execute_reply.started": "2022-10-18T01:36:07.275190Z"
    },
    "id": "S0ywCOsT1xfE",
    "outputId": "a2bf5c39-c99e-47c3-83d1-54abf3d6b171"
   },
   "outputs": [],
   "source": [
    "categorical_nuniq_identification.cat_nuniq_less_than_10(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:08.030142Z",
     "iopub.status.busy": "2022-10-18T01:36:08.029764Z",
     "iopub.status.idle": "2022-10-18T01:36:11.253335Z",
     "shell.execute_reply": "2022-10-18T01:36:11.252069Z",
     "shell.execute_reply.started": "2022-10-18T01:36:08.030117Z"
    },
    "id": "8wNqqRrP1xfE",
    "outputId": "ab39c798-dfe2-45dc-e2a8-6b5c41f735ca"
   },
   "outputs": [],
   "source": [
    "# To find the distribution of different values in below features of dataset\n",
    "# It includes both pie chart and bar chart to visualize value counts of different values of features in dataset\n",
    "cat_col = ['Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status']\n",
    "count = 1\n",
    "for cols in cat_col:\n",
    "    plt.subplot(5, 2, count)\n",
    "    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%',radius=1,textprops={'fontsize': 10} )\n",
    "    count +=1\n",
    "    plt.subplot(5, 2, count)\n",
    "    plt.tight_layout()\n",
    "    sns.countplot(cols, data=df)\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(12,25)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xticks(rotation=45)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:11.255727Z",
     "iopub.status.busy": "2022-10-18T01:36:11.255335Z",
     "iopub.status.idle": "2022-10-18T01:36:19.996441Z",
     "shell.execute_reply": "2022-10-18T01:36:19.995143Z",
     "shell.execute_reply.started": "2022-10-18T01:36:11.255693Z"
    },
    "id": "Zf34ioFO1xfE",
    "outputId": "fe6eaae1-5857-4a3e-e986-0a74b8224be1"
   },
   "outputs": [],
   "source": [
    "# catplot to visualize the trend of purchase against age, city_category and gender\n",
    "sns.catplot(x='Age',y='Purchase',kind='point',data=df,\n",
    "            col='City_Category',hue='Gender', \n",
    "            order=age_dict.values())\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(25,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:19.998090Z",
     "iopub.status.busy": "2022-10-18T01:36:19.997836Z",
     "iopub.status.idle": "2022-10-18T01:36:20.472348Z",
     "shell.execute_reply": "2022-10-18T01:36:20.471413Z",
     "shell.execute_reply.started": "2022-10-18T01:36:19.998067Z"
    },
    "id": "oQutsz8i1xfF",
    "outputId": "a1b20b4f-4863-4ae1-8d72-64078e879fe6"
   },
   "outputs": [],
   "source": [
    "# Boxplot of Age Vs Purchase in ascending order of purchase\n",
    "age_mode = df.groupby(['Age'])['Purchase'].median().sort_values()\n",
    "sns.boxenplot(x=df['Age'], y=df['Purchase'], order=list(age_mode.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:20.474173Z",
     "iopub.status.busy": "2022-10-18T01:36:20.473694Z",
     "iopub.status.idle": "2022-10-18T01:36:21.191599Z",
     "shell.execute_reply": "2022-10-18T01:36:21.190062Z",
     "shell.execute_reply.started": "2022-10-18T01:36:20.474147Z"
    },
    "id": "m2FhSQWB1xfF",
    "outputId": "d076bab5-58de-42de-c96c-2122bd07e49a"
   },
   "outputs": [],
   "source": [
    "def purchase_vs_categorical(df, purchase, other_cat):\n",
    "    '''\n",
    "    Function to create visualization of target variable vs provided given variable. This function is to\n",
    "    calculate the sum of purchases for any given category.\n",
    "    :param df: Source dataframe\n",
    "    :param purchase: Target Variable 'Purchase'\n",
    "    :param other_cat: Categorical Variable which we need to plot against.\n",
    "    '''\n",
    "    part_df = df.groupby(other_cat).sum()[purchase].sort_values(ascending=False).head(10)\n",
    "    part_df.index = part_df.index.astype('str')\n",
    "    cap =  'Purchase vs ' + other_cat\n",
    "    fig = go.Bar(x=part_df.index, y=part_df.values)\n",
    "    return fig\n",
    "\n",
    "# creating subplots to visualize sum of purchases of different categories in various features of dataset\n",
    "\n",
    "fig = make_subplots(rows=4, cols=2, \n",
    "                   subplot_titles=['Purchase Vs User_ID',\n",
    "                                  'Purchase Vs City_Category',\n",
    "                                  'Purchase Vs Occupation',\n",
    "                                  'Purchase Vs Gender',\n",
    "                                  'Purchase Vs Age',\n",
    "                                  'Purchase Vs Stay_In_Current_City_Years',\n",
    "                                  'Purchase Vs Marital_Status'])\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'User_ID' ),\n",
    "             row=1,col=1)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'City_Category' ),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'Occupation' ),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'Gender' ),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'Age' ),\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'Stay_In_Current_City_Years' ),\n",
    "              row=3, col=2)\n",
    "\n",
    "fig.add_trace(purchase_vs_categorical(df, 'Purchase', 'Marital_Status' ),\n",
    "              row=4, col=1)\n",
    "\n",
    "fig.update_layout(width=1000, height=1000, title = 'Purchases hold for every Categorical Variables')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:21.194154Z",
     "iopub.status.busy": "2022-10-18T01:36:21.193758Z",
     "iopub.status.idle": "2022-10-18T01:36:21.739319Z",
     "shell.execute_reply": "2022-10-18T01:36:21.738274Z",
     "shell.execute_reply.started": "2022-10-18T01:36:21.194123Z"
    },
    "id": "hImgjnVL1xfF",
    "outputId": "69ddb2b9-1d84-4e2c-dc99-acf0ef891702"
   },
   "outputs": [],
   "source": [
    "# plotting the scatter chart to understand distribultion of records with respect to occupation and gender\n",
    "fig = px.histogram(df, x='Occupation', width=800, height=400, facet_col=\"Gender\",\n",
    "                  title='Distribution of Records with respect to Occupation and Gender')\n",
    "# showing the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:21.740668Z",
     "iopub.status.busy": "2022-10-18T01:36:21.740429Z",
     "iopub.status.idle": "2022-10-18T01:36:23.154672Z",
     "shell.execute_reply": "2022-10-18T01:36:23.153801Z",
     "shell.execute_reply.started": "2022-10-18T01:36:21.740645Z"
    },
    "id": "tMENn43d1xfF",
    "outputId": "0a1ed108-4382-45bf-9099-ad1b15f2751f"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "df[df['Gender']=='M']['Age'].value_counts().sort_values().plot(kind='pie',autopct=lambda p:'{:.2f}% ({:.0f})'.format(p,(p/100)*(df[df['Gender']=='M']['Age'].value_counts().sort_values()).sum()),explode=[0,0,0,0,0,0,0.05])\n",
    "plt.title('Male Gender - Age wise data distribution')\n",
    "\n",
    "plt.subplot(122)\n",
    "df[df['Gender']=='F']['Age'].value_counts().sort_values().plot(kind='pie',autopct=lambda p:'{:.2f}% ({:.0f})'.format(p,(p/100)*(df[df['Gender']=='F']['Age'].value_counts().sort_values()).sum()),explode=[0,0,0,0,0,0,0.05])\n",
    "plt.title('Male Gender - Age wise data distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:23.158113Z",
     "iopub.status.busy": "2022-10-18T01:36:23.155816Z",
     "iopub.status.idle": "2022-10-18T01:36:25.328405Z",
     "shell.execute_reply": "2022-10-18T01:36:25.327372Z",
     "shell.execute_reply.started": "2022-10-18T01:36:23.158077Z"
    },
    "id": "7Uezi2IP1xfG",
    "outputId": "cfce73a5-8bb7-4a1c-abd7-3f2de9a1b51f"
   },
   "outputs": [],
   "source": [
    "# plotting a distribution plot for the target variable\n",
    "plt.rcParams['figure.figsize'] = (20, 7)\n",
    "sns.distplot(df['Purchase'], color = 'green', fit = norm)\n",
    "\n",
    "# fitting the target variable to the normal curve \n",
    "mean, std = norm.fit(df['Purchase']) \n",
    "print(\"The mu {} and Sigma {} for the curve\\n\".format(mean, std))\n",
    "\n",
    "plt.title('A distribution plot to represent the distribution of Purchase')\n",
    "plt.legend(['Normal Distribution ($mean$: {}, $std$: {}'.format(mean, std)], loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3LAh_Zq1xfG"
   },
   "source": [
    "## Data Pre-Processing:\n",
    "\n",
    "**Handling Missing Values**\n",
    "* 1st level: Replacing na's with mode grouping by 'Gender', 'Age', 'Occupation'\n",
    "* 2nd level: Replacing na's with mode grouping by 'Gender', 'Age'\n",
    "* 3rd level: Replacing na's with mode grouping by 'Age'\n",
    "\n",
    "**Handling Outliers**\n",
    "* Capped outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:25.330344Z",
     "iopub.status.busy": "2022-10-18T01:36:25.329973Z",
     "iopub.status.idle": "2022-10-18T01:36:25.343366Z",
     "shell.execute_reply": "2022-10-18T01:36:25.341947Z",
     "shell.execute_reply.started": "2022-10-18T01:36:25.330315Z"
    },
    "id": "N5PYom6Q1xfG"
   },
   "outputs": [],
   "source": [
    "class initial_data_pre_processing:\n",
    "    '''\n",
    "    This class is to handle missing values in product_category_1 and product_category_2.\n",
    "    1st level: Replacing na's with mode grouping by 'Gender', 'Age', 'Occupation'\n",
    "    2nd level: Replacing na's with mode grouping by 'Gender', 'Age'\n",
    "    3rd level: Replacing na's with mode grouping by 'Age'\n",
    "    '''\n",
    "    @classmethod\n",
    "    def handling_missing_data(self, df, thres=0.95):\n",
    "        df = df.copy()\n",
    "        df = self.impute_na(df)\n",
    "        print(f'\\n\\nInitial Pre-Precessing Completed')\n",
    "        print(f'\\n\\nShape of dataset after initial pre-processing is...{df.shape}')\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def impute_na(self, df):\n",
    "        print(f'Started Handling Missing Values...\\n')\n",
    "        df = df.copy()\n",
    "        print(f'\\nTotal null values before imputing: ', df.isnull().sum().sum())\n",
    "        df_na = understanding_data.na_count(df, 0)\n",
    "        all_col_na = df_na.index.tolist()\n",
    "        for col in all_col_na:\n",
    "            try:\n",
    "                df[col] = df.groupby(['Gender', 'Age', 'Occupation'], sort = False)[col].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "            except:\n",
    "                if df.isnull().sum().sum() > 0:\n",
    "                    try:\n",
    "                        df[col] = df.groupby(['Gender', 'Age'], sort = False)[col].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "                    except:\n",
    "                        df[col] = df.groupby(['Age'], sort = False)[col].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "                        df[col] = df.groupby(['Gender'], sort = False)[col].apply(lambda x: x.fillna(x.mode().iloc[0]))        \n",
    "        print(f'\\n\\nTotal null value count after imputing: ', df.isnull().sum().sum())\n",
    "        print(f'\\n\\nCompleted Handling Missing Values....')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:25.345397Z",
     "iopub.status.busy": "2022-10-18T01:36:25.345039Z",
     "iopub.status.idle": "2022-10-18T01:36:26.853941Z",
     "shell.execute_reply": "2022-10-18T01:36:26.852902Z",
     "shell.execute_reply.started": "2022-10-18T01:36:25.345371Z"
    },
    "id": "wL4R8jwL1xfG",
    "outputId": "663ea9da-71fa-4ff0-9a80-ecfb8d9be13f"
   },
   "outputs": [],
   "source": [
    "# oh_col = ['Age', 'City_Category', 'Gender', 'Marital_Status', 'Stay_In_Current_City_Years']\n",
    "df = initial_data_pre_processing.handling_missing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:26.857670Z",
     "iopub.status.busy": "2022-10-18T01:36:26.857192Z",
     "iopub.status.idle": "2022-10-18T01:36:26.869725Z",
     "shell.execute_reply": "2022-10-18T01:36:26.868653Z",
     "shell.execute_reply.started": "2022-10-18T01:36:26.857644Z"
    },
    "id": "jsJNe-941xfG"
   },
   "outputs": [],
   "source": [
    "class handling_outliers:\n",
    "    '''\n",
    "    This class is to cap the outliers present in features 'Occupation', 'Stay_In_Current_City_Years', 'Product_Category_1',\n",
    "    'Product_Category_2', 'Product_Category_3'.\n",
    "    '''\n",
    "    @classmethod\n",
    "    def outliers_pipeline(self, df):\n",
    "        df = df.copy()\n",
    "        print(f'Outlier Capping.....\\n\\n')\n",
    "        features = [col for col in df.columns if col!= 'Purchase']\n",
    "        out_features = ['Occupation', 'Stay_In_Current_City_Years', 'Product_Category_1',\n",
    "                       'Product_Category_2', 'Product_Category_3']\n",
    "        print(f\"Outliers plot before capping..\\n\")\n",
    "        # self.visualize_outliers(df, out_features)\n",
    "        df = self.capping_outliers(df, out_features) \n",
    "        print(f'\\nCapping outliers completed')\n",
    "        # self.visualize_outliers(df, out_features)\n",
    "        print(f\"\\n\\nOutliers plot after capping\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def visualize_outliers(self, df, outlier_column):\n",
    "        fig = make_subplots(rows=3, cols=2, \n",
    "                           subplot_titles=['Occupation',\n",
    "                                          'Stay_In_Current_City_Years',\n",
    "                                          'Age',\n",
    "                                          'Product_Category_1',\n",
    "                                          'Product_Category_2',\n",
    "                                          'Product_Category_3'])\n",
    "        fig.add_trace(go.Box(y=df.Occupation.values),\n",
    "                     row=1,col=1)\n",
    "\n",
    "        fig.add_trace(go.Box(y=df.Stay_In_Current_City_Years.values),\n",
    "                      row=1, col=2)\n",
    "\n",
    "        fig.add_trace(go.Box(y=df.Age.values),\n",
    "                      row=2, col=1)\n",
    "\n",
    "        fig.add_trace(go.Box(y=df.Product_Category_1.values),\n",
    "                      row=2, col=2)\n",
    "        \n",
    "        fig.add_trace(go.Box(y=df.Product_Category_2.values),\n",
    "                      row=3, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Box(y=df.Product_Category_3.values),\n",
    "                      row=3, col=2)\n",
    "\n",
    "        fig.update_layout(width=1000, height=1000, title = 'Outliers Plot')\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    @classmethod\n",
    "    def capping_outliers(self, df, num_col):\n",
    "        df = df.copy()\n",
    "        df[num_col] = df[num_col].astype(int)\n",
    "        for col in num_col:\n",
    "            percentiles = df[col].quantile([0.10, 0.90]).values\n",
    "            df[col][df[col] <= percentiles[0]] = percentiles[0]\n",
    "            df[col][df[col] >= percentiles[1]] = percentiles[1]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:26.871632Z",
     "iopub.status.busy": "2022-10-18T01:36:26.871326Z",
     "iopub.status.idle": "2022-10-18T01:36:27.097328Z",
     "shell.execute_reply": "2022-10-18T01:36:27.096402Z",
     "shell.execute_reply.started": "2022-10-18T01:36:26.871604Z"
    },
    "id": "E8qKaUKI1xfH",
    "outputId": "959bc4e5-a302-494b-d47f-3cfef0d95cb7"
   },
   "outputs": [],
   "source": [
    "df = handling_outliers.outliers_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY7T1MPL1xfH"
   },
   "source": [
    "## Feature Selection and Engineering\n",
    "\n",
    "* **One Hot Encoding**: Applied one hot encoding to following features 'Age', 'City_Category', 'Gender', 'Marital_Status', 'Stay_In_Current_City_Years'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:27.099366Z",
     "iopub.status.busy": "2022-10-18T01:36:27.098523Z",
     "iopub.status.idle": "2022-10-18T01:36:27.107866Z",
     "shell.execute_reply": "2022-10-18T01:36:27.106688Z",
     "shell.execute_reply.started": "2022-10-18T01:36:27.099334Z"
    },
    "id": "ueo7b6QH1xfH"
   },
   "outputs": [],
   "source": [
    "class one_hot_encoding_category:   \n",
    "    '''\n",
    "    This class is to perform one-hot encoding to categorical columns that we provide as input.\n",
    "    '''\n",
    "    @classmethod\n",
    "    def oh_pipeline(self, df, object_cols):\n",
    "        df, dict_feature = self.one_hot_encoding(df, object_cols)\n",
    "        df = self.changing_dtype_final(df, df.columns)\n",
    "        return df, dict_feature\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def one_hot_encoding(self, df, object_cols):\n",
    "        # Apply one-hot encoder to each column with categorical data\n",
    "        OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(df[object_cols]))\n",
    "        OH_cols_train.columns = OH_encoder.get_feature_names_out()\n",
    "\n",
    "        # One-hot encoding removed index; put it back\n",
    "        OH_dict = {}\n",
    "        for col in obj_cols:\n",
    "            OH_dict[col] = [x for x in OH_cols_train.columns if x.startswith(col)]\n",
    "\n",
    "        # Remove categorical columns (will replace with one-hot encoding)\n",
    "        num_X_train = df.drop(object_cols, axis=1)\n",
    "\n",
    "        # Add one-hot encoded columns to numerical features\n",
    "        OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "        return OH_X_train, OH_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:27.110924Z",
     "iopub.status.busy": "2022-10-18T01:36:27.109254Z",
     "iopub.status.idle": "2022-10-18T01:36:27.837568Z",
     "shell.execute_reply": "2022-10-18T01:36:27.836933Z",
     "shell.execute_reply.started": "2022-10-18T01:36:27.110851Z"
    },
    "id": "VEEFfF_n1xfI",
    "outputId": "75feb6c4-af4c-4b4b-cf88-cb19a0dc19bd"
   },
   "outputs": [],
   "source": [
    "print(f\"One Hot Encoding...\")\n",
    "obj_cols = ['Age', 'City_Category', 'Gender', 'Marital_Status', 'Stay_In_Current_City_Years']\n",
    "print(f\"\\n\\nColumns to be one-hot encoded: {obj_cols}\")\n",
    "print(f\"\\n\\nNumber of Columns before one-hot encoding {df.shape[1]}\")\n",
    "df, OH_dict = one_hot_encoding_category.one_hot_encoding(df, obj_cols)\n",
    "print(f\"\\n\\nNumber of Columns after one-hot encoding {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:27.839631Z",
     "iopub.status.busy": "2022-10-18T01:36:27.838546Z",
     "iopub.status.idle": "2022-10-18T01:36:27.846505Z",
     "shell.execute_reply": "2022-10-18T01:36:27.844960Z",
     "shell.execute_reply.started": "2022-10-18T01:36:27.839569Z"
    },
    "id": "9Qtxkkoa1xfI"
   },
   "outputs": [],
   "source": [
    "def changing_dtype_final(df):\n",
    "    '''\n",
    "    This function is to change the data type of different features with float data type to int data type\n",
    "    :param df: Data frame which needs data type conversion\n",
    "    :return df: Data type converted data frame.\n",
    "    '''\n",
    "    conv_col = [col for col in df.columns if df[col].dtype == 'float']\n",
    "    print(f\"Changing Datatype from Float to int.....\\n\")\n",
    "    for col in conv_col:\n",
    "        df[col] = df[col].astype(int)\n",
    "    print(f'\\nConverted {len(conv_col)} features from float to Int...\\n\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:27.848690Z",
     "iopub.status.busy": "2022-10-18T01:36:27.848269Z",
     "iopub.status.idle": "2022-10-18T01:36:28.048122Z",
     "shell.execute_reply": "2022-10-18T01:36:28.046361Z",
     "shell.execute_reply.started": "2022-10-18T01:36:27.848659Z"
    },
    "id": "BNSkdEp91xfI",
    "outputId": "c78590ba-6d1c-4d46-dc86-3cdfff30f2b8"
   },
   "outputs": [],
   "source": [
    "df = changing_dtype_final(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:28.050035Z",
     "iopub.status.busy": "2022-10-18T01:36:28.049683Z",
     "iopub.status.idle": "2022-10-18T01:36:30.784073Z",
     "shell.execute_reply": "2022-10-18T01:36:30.782918Z",
     "shell.execute_reply.started": "2022-10-18T01:36:28.050006Z"
    },
    "id": "htjvD2Om1xfI",
    "outputId": "83a8e4b9-7616-439c-afd8-2bf22e0a4e56"
   },
   "outputs": [],
   "source": [
    "# Heat map to find the features with high correlation coefficient\n",
    "sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "plt.title('Correlation Heat Map to understand multi-collinearity')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:30.785730Z",
     "iopub.status.busy": "2022-10-18T01:36:30.785436Z",
     "iopub.status.idle": "2022-10-18T01:36:30.799242Z",
     "shell.execute_reply": "2022-10-18T01:36:30.796658Z",
     "shell.execute_reply.started": "2022-10-18T01:36:30.785704Z"
    },
    "id": "OQQtuJWi1xfK"
   },
   "outputs": [],
   "source": [
    "class Feature_Engineering:\n",
    "    '''\n",
    "    This class is to drop columns 'User_ID' and 'Product_ID', and to remove highly correlated features.\n",
    "    '''\n",
    "    @classmethod\n",
    "    def feature_engineering(self, df):\n",
    "        # removing user and product ID as it does not hold\n",
    "        print(f'\\nTotal number of Columns which are present before removing user_id and product_id are {len(df.columns)}\\n')\n",
    "        df = df.drop(['User_ID', 'Product_ID'], axis=1)\n",
    "        print(f'\\nUser ID and Product ID are removed from dataset\\n')\n",
    "        print(f'\\nTotal number of Columns which are present after removing user_id and product_id are {len(df.columns)}\\n')\n",
    "        df = self.dropping_high_correlated_feature(df)\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def dropping_high_correlated_feature(self, df):\n",
    "        print(f'\\nRemoving Correlated Features.....\\n')\n",
    "        cor_matrix = df.corr().abs()\n",
    "        upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "        print(f'\\nColumns which are removed due to high correlation: {to_drop}\\n')\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        print(f'\\nTotal number of Columns which are present after removing highly correlated features are {len(df.columns)}')\n",
    "        return df  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:30.801591Z",
     "iopub.status.busy": "2022-10-18T01:36:30.801127Z",
     "iopub.status.idle": "2022-10-18T01:36:31.522035Z",
     "shell.execute_reply": "2022-10-18T01:36:31.520794Z",
     "shell.execute_reply.started": "2022-10-18T01:36:30.801550Z"
    },
    "id": "ql3l3D8f1xfK",
    "outputId": "7c06a9cb-dfac-48d3-eba0-4e895e026b18"
   },
   "outputs": [],
   "source": [
    "df = Feature_Engineering.feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:31.526532Z",
     "iopub.status.busy": "2022-10-18T01:36:31.526227Z",
     "iopub.status.idle": "2022-10-18T01:36:33.938813Z",
     "shell.execute_reply": "2022-10-18T01:36:33.937476Z",
     "shell.execute_reply.started": "2022-10-18T01:36:31.526509Z"
    },
    "id": "5o2blTyv1xfK",
    "outputId": "49f79b37-97a5-48dd-e62c-c493300d9980"
   },
   "outputs": [],
   "source": [
    "# Correlation plot after removing highly correlated features\n",
    "sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "plt.title('Correlation Heat Map to understand multi-collinearity After removing correlated Features')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:33.940732Z",
     "iopub.status.busy": "2022-10-18T01:36:33.940413Z",
     "iopub.status.idle": "2022-10-18T01:36:59.155011Z",
     "shell.execute_reply": "2022-10-18T01:36:59.154267Z",
     "shell.execute_reply.started": "2022-10-18T01:36:33.940703Z"
    },
    "id": "1AT2n2891xfK",
    "outputId": "a4fe3707-054c-46fe-88a1-89b94f31594d"
   },
   "outputs": [],
   "source": [
    "def calculate_vif(data):\n",
    "    '''\n",
    "    This function is to calculate Variation Inflation Factor in order to eliminate multicollinearity problem created by\n",
    "    one-hot encoding problem.\n",
    "    '''\n",
    "    vif_df = pd.DataFrame(columns = ['Feature', 'Vif', 'Feature Percentage'])\n",
    "    x_var_names = data.columns\n",
    "    for i in range(0, x_var_names.shape[0]):\n",
    "        y = data[x_var_names[i]]\n",
    "        x = data[x_var_names.drop([x_var_names[i]])]\n",
    "        r_squared = sm.OLS(y,x).fit().rsquared\n",
    "        vif = round(1/(1-r_squared),2)\n",
    "        vif_df.loc[i] = [x_var_names[i], vif, df[df[x_var_names[i]] == 1].shape[0]/df.shape[0]]\n",
    "    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)\n",
    "\n",
    "calculate_vif(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:36:59.157000Z",
     "iopub.status.busy": "2022-10-18T01:36:59.156248Z",
     "iopub.status.idle": "2022-10-18T01:37:15.169310Z",
     "shell.execute_reply": "2022-10-18T01:37:15.168381Z",
     "shell.execute_reply.started": "2022-10-18T01:36:59.156965Z"
    },
    "id": "F4nr4V3R1xfK",
    "outputId": "6e52044d-7cbf-42a2-a3c4-27831c5afa21"
   },
   "outputs": [],
   "source": [
    "# df.drop(['Age_Older above 55'], axis=1, inplace=True)\n",
    "# df.drop(['Stay_In_Current_City_Years_0'], axis=1, inplace=True)\n",
    "# df.drop(['City_Category_A'], axis=1, inplace=True)\n",
    "# df.drop(['Product_Category_3'], axis=1, inplace=True)\n",
    "# dropping the colum which reduces vif from infinity\n",
    "df.drop(['Age_Older above 55', 'Stay_In_Current_City_Years_0', 'City_Category_A', 'Product_Category_3'], axis=1, inplace=True)\n",
    "calculate_vif(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:37:15.171531Z",
     "iopub.status.busy": "2022-10-18T01:37:15.170961Z",
     "iopub.status.idle": "2022-10-18T01:37:15.351001Z",
     "shell.execute_reply": "2022-10-18T01:37:15.350125Z",
     "shell.execute_reply.started": "2022-10-18T01:37:15.171495Z"
    },
    "id": "9vLlxCn91xfL",
    "outputId": "d0d3ea95-853b-4ef8-9101-8cd45efc7cbc"
   },
   "outputs": [],
   "source": [
    "# removing duplicates from the dataframe\n",
    "print(f\"\\nTotal data found in dataset before dropping duplicates: {df.shape[0]}\\n\")\n",
    "df.drop_duplicates(keep='first', inplace = True)\n",
    "print(f\"\\nTotal data found in dataset before dropping duplicates: {df.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-18T01:37:15.352405Z",
     "iopub.status.busy": "2022-10-18T01:37:15.352102Z",
     "iopub.status.idle": "2022-10-18T01:37:15.497443Z",
     "shell.execute_reply": "2022-10-18T01:37:15.496246Z",
     "shell.execute_reply.started": "2022-10-18T01:37:15.352375Z"
    },
    "id": "rQAoxor51xfL"
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Purchase']\n",
    "y= df['Purchase']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,random_state=123,test_size=0.25,\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:37:49.300811Z",
     "iopub.status.busy": "2022-10-18T01:37:49.299508Z",
     "iopub.status.idle": "2022-10-18T01:37:49.521834Z",
     "shell.execute_reply": "2022-10-18T01:37:49.521021Z",
     "shell.execute_reply.started": "2022-10-18T01:37:49.300758Z"
    },
    "id": "DS8ia3Jw1xfL",
    "outputId": "c8c1a8a7-1399-4348-914f-65924e6956ce"
   },
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "print(f'Linear Regression Model....\\n')\n",
    "linear_model = Pipeline(steps=[('Standardization', StandardScaler()),\n",
    "                              ('linear_model', LinearRegression())\n",
    "                             ])\n",
    "# Preprocessing of training data, fit model \n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:37:58.553697Z",
     "iopub.status.busy": "2022-10-18T01:37:58.552685Z",
     "iopub.status.idle": "2022-10-18T01:37:58.574256Z",
     "shell.execute_reply": "2022-10-18T01:37:58.573429Z",
     "shell.execute_reply.started": "2022-10-18T01:37:58.553639Z"
    },
    "id": "B_457Hfn1xfL",
    "outputId": "b026414f-f04f-4c69-a237-adf13e7b75bb"
   },
   "outputs": [],
   "source": [
    "# Predicting the purchase value on test dataset\n",
    "predictions = linear_model.predict(X_test)\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Predicted labels: ', np.round(predictions)[:10])\n",
    "print('Actual labels   : ' , y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:38:03.755272Z",
     "iopub.status.busy": "2022-10-18T01:38:03.754809Z",
     "iopub.status.idle": "2022-10-18T01:38:04.106633Z",
     "shell.execute_reply": "2022-10-18T01:38:04.105548Z",
     "shell.execute_reply.started": "2022-10-18T01:38:03.755234Z"
    },
    "id": "bcIsoMrl1xfL",
    "outputId": "ab0c142d-0548-4f5f-8656-72981da0aa42"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Black Friday Purchase Predictions using Linear Regression')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "execution": {
     "iopub.execute_input": "2022-10-18T01:38:18.771570Z",
     "iopub.status.busy": "2022-10-18T01:38:18.771163Z",
     "iopub.status.idle": "2022-10-18T01:39:47.788345Z",
     "shell.execute_reply": "2022-10-18T01:39:47.787212Z",
     "shell.execute_reply.started": "2022-10-18T01:38:18.771537Z"
    },
    "id": "CJeATneI1xfL",
    "outputId": "a132e5fd-5c2e-4b52-dbe5-09a7b3dca879"
   },
   "outputs": [],
   "source": [
    "# Train the  random forest regressor model\n",
    "print(f\"Random Forest Regressor Model.....\\n\")\n",
    "rf_model = Pipeline(steps=[('Standardization', StandardScaler()),\n",
    "                              ('linear_model', RandomForestRegressor())\n",
    "                             ])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Black Friday Purchase Predictions using Random Forest Regressor')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
